#Machine Learning Gradient Descent
##引言
上一节课说明了多变量情况下的表示方式，在梯度下降的方面，单变量与多变量也有所不同

##单变量
###代价函数
$J(\theta_0,\theta_1)=\frac{1}{2m}\Sigma^m_{i=1}(h(x_i)-y_i)^2$

由于没有更多的变量，代价函数实际就是预测值与样本值的方差
###梯度下降函数
$\theta_i=\theta_i-\alpha\frac{\partial}{\partial \theta_i}J(\theta_0,\theta_1)$

如何理解这个公式呢

这需要先回忆一下我们为什么要进行梯度下降，代价函数的值在某种程度上可以表示假设函数与样本值的偏差，那么在代价函数取到最小值时，假设函数应该为目前样本所能找到的最好结果。

但是，由于代价函数通常只能对一个一个值进行计算，为了得到一个未知函数的最小值，只能去找目前可见的最好的结果。

就像是要下山，但山上有雾，那么想要最快找到下山的路，就只能从现在向山下最陡的方向移动。

在一元函数中，形容一个函数陡峭程度的值就是斜率（其实就是一元函数的梯度），想要向着最低点移动，就要将这个点（系数的值）沿着斜率进行移动，移动的量取决于斜率的大小（就好比是坡陡，走得快一样），以及$\alpha$的值（$\alpha$只是一个每次梯度下降步进的大小，实际上，梯度下降的值的大小应该是没有具体意义的，仅仅是梯度小移动小）。




梯度下降的目的是找到代价函数的极小值



$$
	\theta=
	\left(
	\begin{matrix}
	{\theta_0} & {\theta_1} & ... & {\theta_n}
	\end{matrix}
	\right)
$$

$$
	x=
	\left(
	\begin{matrix}
	{x_0} & {x_1} & ... & {x_n}
	\end{matrix}
	\right)
$$

那么$h_\theta(x)=\theta^Tx$
